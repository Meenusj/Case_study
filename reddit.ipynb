{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyORfoK8l+EC8MiozpJAMETo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/Case_study/blob/main/reddit.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0CPwEYY6u74",
        "outputId": "6c620465-c9d9-4f34-a735-be3d62f2168e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m225.3/227.1 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv('/content/reddit_filtered_dataset.csv')\n",
        "df.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "ZjtHxfHK6OFG",
        "outputId": "2fcbbbc0-b317-4867-e779-6c5bf04034c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                Data  Labels\n",
              "0  Of course! The relevant passage is: \"As to you...       0\n",
              "1  I've actually written a bit about this before....       0\n",
              "2  The Waco standoff in 1993 was a complex and co...       1\n",
              "3  Incredible answer. This was very well sourced ...       0\n",
              "4  The identification of the ruins of Troy is bas...       1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-55400736-9ed8-49ad-8ac6-c45ccedef5e5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Data</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Of course! The relevant passage is: \"As to you...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I've actually written a bit about this before....</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The Waco standoff in 1993 was a complex and co...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Incredible answer. This was very well sourced ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The identification of the ruins of Troy is bas...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-55400736-9ed8-49ad-8ac6-c45ccedef5e5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-55400736-9ed8-49ad-8ac6-c45ccedef5e5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-55400736-9ed8-49ad-8ac6-c45ccedef5e5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-883e4f81-cec0-441c-b98b-afac84487b69\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-883e4f81-cec0-441c-b98b-afac84487b69')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-883e4f81-cec0-441c-b98b-afac84487b69 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 6513,\n  \"fields\": [\n    {\n      \"column\": \"Data\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6489,\n        \"samples\": [\n          \"yes and no.  The severity of the problem has been reduced a lot because of air emission rules limiting discharge of acid-forming gases (sulfur and nitrogen oxides are the main problem).  Restrictions such as requiring the use of \\\"clean coal\\\" (low sulfur coal) or use of air scrubbers for discharge stacks have resulted in significant decreases in total discharges of acid-forming compounds, and thus the precipitation is less acidic.  There is still some man-caused acidity that is mostly a problem in the northeast, but it is a lot less severe than it was only a couple decades ago. It isn't gone, just no longer really bad and getting worse. Unless you live in China.  It is getting worse there.\",\n          \"I work overnights.  Midnight to noon. Everything shifts to as it was during the day shift:  - The times I am most productive tend to be about 4 hours after arriving at work.  - My afternoon dump is about 5 hours after arriving at work.  - My sleepiest time is about 6 hours after arriving at work.    - Morning wood has shifted to when I wake up at night (it was still at 6 in the morning when I started the night shift, very inconvenient).  - I can take a 15 minute nap at anytime, 3am, 6am, 9am - even in daylight with windows.  - When I get hungry is a little off because I eat when I am tired, which is more often.  But breakfast 12:30am, lunch 6am, Dinner is after noon.\",\n          \"Mexico's transition from a heavily armed country to one with strict gun control measures and only one gun store can be attributed to a combination of historical, cultural, and political factors. Here are a few key points:1. Historical Context: Mexico has a long history of armed conflicts and revolutions, resulting in a proliferation of firearms throughout the country. From the Mexican Revolution in the early 20th century to ongoing drug-related violence, the presence of weapons has been deeply embedded in Mexican society. 2.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Labels\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "quO-Casp5tl6",
        "outputId": "26ee5f69-9e9c-4395-cf27-d79874ef05ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "72/72 [==============================] - 4s 6ms/step - loss: 0.5344 - accuracy: 0.7752 - val_loss: 0.4408 - val_accuracy: 0.7871\n",
            "Epoch 2/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.3948 - accuracy: 0.8024 - val_loss: 0.3471 - val_accuracy: 0.8403\n",
            "Epoch 3/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.3160 - accuracy: 0.8592 - val_loss: 0.2808 - val_accuracy: 0.8792\n",
            "Epoch 4/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.2651 - accuracy: 0.8844 - val_loss: 0.2531 - val_accuracy: 0.9017\n",
            "Epoch 5/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.2305 - accuracy: 0.9052 - val_loss: 0.2455 - val_accuracy: 0.9028\n",
            "Epoch 6/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.2102 - accuracy: 0.9118 - val_loss: 0.2429 - val_accuracy: 0.9038\n",
            "Epoch 7/10\n",
            "72/72 [==============================] - 0s 4ms/step - loss: 0.2010 - accuracy: 0.9162 - val_loss: 0.2309 - val_accuracy: 0.9069\n",
            "Epoch 8/10\n",
            "72/72 [==============================] - 0s 5ms/step - loss: 0.1811 - accuracy: 0.9307 - val_loss: 0.2358 - val_accuracy: 0.9048\n",
            "Epoch 9/10\n",
            "72/72 [==============================] - 0s 5ms/step - loss: 0.1636 - accuracy: 0.9373 - val_loss: 0.2328 - val_accuracy: 0.9079\n",
            "Epoch 10/10\n",
            "72/72 [==============================] - 0s 5ms/step - loss: 0.1461 - accuracy: 0.9443 - val_loss: 0.2391 - val_accuracy: 0.9079\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.2302 - accuracy: 0.9079\n",
            "Test accuracy: 0.9078812599182129\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Confusion Matrix:\n",
            "[[740  40]\n",
            " [ 50 147]]\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        1          1\n",
            "1        1          1\n",
            "2        0          0\n",
            "3        0          0\n",
            "4        0          0\n",
            "5        1          0\n",
            "6        1          0\n",
            "7        0          0\n",
            "8        0          0\n",
            "9        0          0\n",
            "10       1          0\n",
            "11       0          0\n",
            "12       0          0\n",
            "13       0          0\n",
            "14       1          1\n",
            "15       0          0\n",
            "16       0          0\n",
            "17       0          0\n",
            "18       0          0\n",
            "19       0          0\n",
            "20       0          0\n",
            "21       0          0\n",
            "22       0          0\n",
            "23       0          0\n",
            "24       0          0\n",
            "25       1          0\n",
            "26       0          0\n",
            "27       0          0\n",
            "28       0          0\n",
            "29       0          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "\n",
        "X = df['Data'].values\n",
        "y = df['Labels'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train, validation, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=10,\n",
        "                        batch_size=64,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "# Assuming df is already loaded and contains 'Data' and 'Labels' columns\n",
        "X = df['Data'].values\n",
        "y = df['Labels'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train, validation, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_cnn.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=15,\n",
        "                        batch_size=32,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50-9uYVS6sPZ",
        "outputId": "c1dfd031-9c82-4b8f-9285-6230b9015536"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "143/143 [==============================] - 2s 5ms/step - loss: 0.4457 - accuracy: 0.7923 - val_loss: 0.3327 - val_accuracy: 0.8536\n",
            "Epoch 2/15\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 0.2810 - accuracy: 0.8826 - val_loss: 0.2491 - val_accuracy: 0.8987\n",
            "Epoch 3/15\n",
            "143/143 [==============================] - 1s 6ms/step - loss: 0.2136 - accuracy: 0.9118 - val_loss: 0.2314 - val_accuracy: 0.9048\n",
            "Epoch 4/15\n",
            "143/143 [==============================] - 1s 5ms/step - loss: 0.1639 - accuracy: 0.9329 - val_loss: 0.2422 - val_accuracy: 0.9048\n",
            "Epoch 5/15\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.1218 - accuracy: 0.9542 - val_loss: 0.2588 - val_accuracy: 0.9069\n",
            "Epoch 6/15\n",
            "143/143 [==============================] - 1s 4ms/step - loss: 0.0876 - accuracy: 0.9669 - val_loss: 0.2748 - val_accuracy: 0.9069\n",
            "31/31 [==============================] - 0s 2ms/step - loss: 0.2393 - accuracy: 0.8987\n",
            "Test accuracy: 0.898669421672821\n",
            "31/31 [==============================] - 0s 1ms/step\n",
            "Confusion Matrix:\n",
            "[[736  44]\n",
            " [ 55 142]]\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        1          1\n",
            "1        1          1\n",
            "2        0          0\n",
            "3        0          0\n",
            "4        0          0\n",
            "5        1          0\n",
            "6        1          0\n",
            "7        0          0\n",
            "8        0          0\n",
            "9        0          0\n",
            "10       1          0\n",
            "11       0          0\n",
            "12       0          0\n",
            "13       0          0\n",
            "14       1          1\n",
            "15       0          0\n",
            "16       0          0\n",
            "17       0          0\n",
            "18       0          0\n",
            "19       0          0\n",
            "20       0          0\n",
            "21       0          0\n",
            "22       0          0\n",
            "23       0          0\n",
            "24       0          0\n",
            "25       1          0\n",
            "26       0          0\n",
            "27       0          0\n",
            "28       0          0\n",
            "29       0          0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "# Convert 'account.type' to 0 and 1\n",
        "df['account.type'] = df['account.type'].map({'human': 0, 'bot': 1})"
      ],
      "metadata": {
        "id": "xahWFJKv-eOY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "# Assuming df is already loaded and contains 'Data' and 'Labels' columns\n",
        "X = df['text'].values\n",
        "y = df['account.type'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train, validation, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.3),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model_cnn.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=15,\n",
        "                        batch_size=32,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-w-eZLqV8YLa",
        "outputId": "bf1d3047-59c1-46fa-bf70-a5cc91301523"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "454/454 [==============================] - 3s 4ms/step - loss: 0.4993 - accuracy: 0.7500 - val_loss: 0.4097 - val_accuracy: 0.8027\n",
            "Epoch 2/15\n",
            "454/454 [==============================] - 2s 4ms/step - loss: 0.4018 - accuracy: 0.8092 - val_loss: 0.3842 - val_accuracy: 0.8194\n",
            "Epoch 3/15\n",
            "454/454 [==============================] - 2s 5ms/step - loss: 0.3587 - accuracy: 0.8296 - val_loss: 0.3720 - val_accuracy: 0.8214\n",
            "Epoch 4/15\n",
            "454/454 [==============================] - 6s 12ms/step - loss: 0.3253 - accuracy: 0.8509 - val_loss: 0.3683 - val_accuracy: 0.8191\n",
            "Epoch 5/15\n",
            "454/454 [==============================] - 3s 6ms/step - loss: 0.2930 - accuracy: 0.8659 - val_loss: 0.3733 - val_accuracy: 0.8230\n",
            "Epoch 6/15\n",
            "454/454 [==============================] - 2s 4ms/step - loss: 0.2618 - accuracy: 0.8820 - val_loss: 0.3827 - val_accuracy: 0.8256\n",
            "Epoch 7/15\n",
            "454/454 [==============================] - 2s 4ms/step - loss: 0.2361 - accuracy: 0.8978 - val_loss: 0.4240 - val_accuracy: 0.8191\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3710 - accuracy: 0.8304\n",
            "Test accuracy: 0.8303830027580261\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "Confusion Matrix:\n",
            "[[1292  267]\n",
            " [ 260 1288]]\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        1          1\n",
            "1        1          1\n",
            "2        1          1\n",
            "3        0          0\n",
            "4        0          0\n",
            "5        1          0\n",
            "6        0          0\n",
            "7        0          1\n",
            "8        1          1\n",
            "9        1          1\n",
            "10       1          1\n",
            "11       0          0\n",
            "12       1          1\n",
            "13       0          0\n",
            "14       1          1\n",
            "15       1          0\n",
            "16       1          1\n",
            "17       0          0\n",
            "18       1          0\n",
            "19       1          1\n",
            "20       0          0\n",
            "21       1          1\n",
            "22       1          1\n",
            "23       1          0\n",
            "24       0          0\n",
            "25       1          1\n",
            "26       0          1\n",
            "27       1          1\n",
            "28       1          1\n",
            "29       1          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dSXwQho_-nMr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}