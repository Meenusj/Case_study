{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1YDQM+Kjm1y0LD4YkTyQu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/Case_study/blob/main/trainlstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install FastText"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HINiGsfrLpui",
        "outputId": "6beb7472-6fd5-4fad-e926-bdb9302d8c19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting FastText\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m993.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from FastText)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from FastText) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from FastText) (1.25.2)\n",
            "Building wheels for collected packages: FastText\n",
            "  Building wheel for FastText (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for FastText: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199773 sha256=56dd248d1f390598e671446bc1af0f6d1f8877ad815bdfefb3cffe2fc6f0a082\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built FastText\n",
            "Installing collected packages: pybind11, FastText\n",
            "Successfully installed FastText-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-72-WFYgLoCi",
        "outputId": "62162268-73a6-42f1-fb3c-354f9474cb45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed data saved to preprocessed_dataset.csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-2315888cc295>:92: DtypeWarning: Columns (0,1,2,3,4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv('df_with_vectors.csv')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 100)               40800     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 100)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 32)                3232      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 5)                 165       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 44197 (172.64 KB)\n",
            "Trainable params: 44197 (172.64 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "4057/4057 [==============================] - 901s 221ms/step - loss: 0.4655 - accuracy: 0.8984 - val_loss: 0.4727 - val_accuracy: 0.8938\n",
            "Epoch 2/10\n",
            "4057/4057 [==============================] - 896s 221ms/step - loss: 0.4582 - accuracy: 0.8986 - val_loss: 0.4737 - val_accuracy: 0.8938\n",
            "Epoch 3/10\n",
            "4057/4057 [==============================] - 897s 221ms/step - loss: 0.4563 - accuracy: 0.8986 - val_loss: 0.4727 - val_accuracy: 0.8938\n",
            "Epoch 4/10\n",
            "4057/4057 [==============================] - 903s 223ms/step - loss: 0.4556 - accuracy: 0.8986 - val_loss: 0.4729 - val_accuracy: 0.8938\n",
            "Epoch 5/10\n",
            "4057/4057 [==============================] - 904s 223ms/step - loss: 0.4550 - accuracy: 0.8986 - val_loss: 0.4730 - val_accuracy: 0.8938\n",
            "Epoch 6/10\n",
            "4057/4057 [==============================] - 900s 222ms/step - loss: 0.4547 - accuracy: 0.8986 - val_loss: 0.4727 - val_accuracy: 0.8938\n",
            "Epoch 7/10\n",
            "4057/4057 [==============================] - 907s 223ms/step - loss: 0.4544 - accuracy: 0.8986 - val_loss: 0.4725 - val_accuracy: 0.8938\n",
            "Epoch 8/10\n",
            "4057/4057 [==============================] - 908s 224ms/step - loss: 0.4543 - accuracy: 0.8986 - val_loss: 0.4756 - val_accuracy: 0.8938\n",
            "Epoch 9/10\n",
            "4057/4057 [==============================] - 910s 224ms/step - loss: 0.4542 - accuracy: 0.8986 - val_loss: 0.4726 - val_accuracy: 0.8938\n",
            "Epoch 10/10\n",
            "4057/4057 [==============================] - 946s 233ms/step - loss: 0.4543 - accuracy: 0.8986 - val_loss: 0.4740 - val_accuracy: 0.8938\n",
            "1268/1268 [==============================] - 100s 79ms/step - loss: 0.4531 - accuracy: 0.8990\n",
            "Loss: 0.4531\n",
            "Accuracy: 0.8990\n",
            "1268/1268 [==============================] - 106s 82ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM model saved.\n",
            "Precision: 0.8082\n",
            "Recall: 0.8990\n",
            "F1 Score: 0.8512\n",
            "Confusion Matrix:\n",
            "[[    0     0     0     0   611]\n",
            " [    0     0     0     0  2055]\n",
            " [    0     0     0     0   758]\n",
            " [    0     0     0     0   673]\n",
            " [    0     0     0     0 36470]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       611\n",
            "           1       0.00      0.00      0.00      2055\n",
            "           2       0.00      0.00      0.00       758\n",
            "           3       0.00      0.00      0.00       673\n",
            "           4       0.90      1.00      0.95     36470\n",
            "\n",
            "    accuracy                           0.90     40567\n",
            "   macro avg       0.18      0.20      0.19     40567\n",
            "weighted avg       0.81      0.90      0.85     40567\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import pickle\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import fasttext\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Dense, LSTM\n",
        "\n",
        "# Load your dataset (replace 'train.csv' with your actual dataset file)\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "# Assuming the correct column name is 'text', replace 'tweet_text' with the actual name\n",
        "def preprocess(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    tokens = [re.sub(r'#\\w+|@\\w+', '', word) for word in tokens]\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply pre-processing to the specified column in the DataFrame\n",
        "df['preprocessed_text'] = df['text'].apply(preprocess)\n",
        "\n",
        "# Save the preprocessed DataFrame to a new CSV file\n",
        "preprocessed_csv_path = 'preprocessed_dataset.csv'\n",
        "df.to_csv(preprocessed_csv_path, index=False)\n",
        "\n",
        "print(f\"Preprocessed data saved to {preprocessed_csv_path}\")\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "df = pd.read_csv(preprocessed_csv_path)\n",
        "\n",
        "# Check for NaN values and replace them with an empty string\n",
        "df['preprocessed_text'].fillna('', inplace=True)\n",
        "df['preprocessed_text'] = df['preprocessed_text'].astype(str)\n",
        "\n",
        "# Tokenize the text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(df['preprocessed_text'])\n",
        "tokenized_text = tokenizer.texts_to_sequences(df['preprocessed_text'])\n",
        "\n",
        "# Padding sequences to the same length\n",
        "max_len = max(len(seq) for seq in tokenized_text)\n",
        "X_padded = pad_sequences(tokenized_text, maxlen=max_len)\n",
        "\n",
        "# Save tokenized text to a text file (required format for FastText)\n",
        "with open('tokenized_text.txt', 'w') as file:\n",
        "    for tokens in tokenized_text:\n",
        "        file.write(\" \".join(map(str, tokens)) + \"\\n\")\n",
        "\n",
        "# Train FastText model\n",
        "model = fasttext.train_unsupervised('tokenized_text.txt', model='skipgram', dim=300, epoch=10)\n",
        "\n",
        "# Save the model\n",
        "model.save_model('fasttext_model_lstm.bin')\n",
        "\n",
        "\n",
        "# Get word vectors for each token\n",
        "word_vectors = []\n",
        "for tokens in tokenized_text:\n",
        "    for word_index in tokens:\n",
        "        word = tokenizer.index_word.get(word_index)\n",
        "        if word:\n",
        "            word_vector = model.get_word_vector(word)\n",
        "            word_vectors.append(word_vector)\n",
        "\n",
        "# Convert word vectors to DataFrame\n",
        "word_vectors_df = pd.DataFrame(word_vectors, columns=[f'feature_{i}' for i in range(300)])\n",
        "\n",
        "\n",
        "\n",
        "# Concatenate the original DataFrame with the word vectors DataFrame\n",
        "df_with_vectors = pd.concat([df, word_vectors_df], axis=1)\n",
        "\n",
        "# Save the DataFrame with additional columns for word vectors\n",
        "df_with_vectors.to_csv('df_with_vectors.csv', index=False)\n",
        "\n",
        "# Load your DataFrame without header\n",
        "df = pd.read_csv('df_with_vectors.csv')\n",
        "\n",
        "# Extract feature columns (assuming they start from column 'feature_0')\n",
        "feature_columns = df.columns[df.columns.str.startswith('feature_')]\n",
        "\n",
        "# Extract features and labels\n",
        "X = df[feature_columns].values\n",
        "y = df['class_type']\n",
        "\n",
        "# Encode class labels using LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test_encoded = train_test_split(X, y_encoded, test_size=0.2, random_state=42)\n",
        "\n",
        "# Reshape the input data to be compatible with Conv1D layer\n",
        "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
        "\n",
        "# Create the LSTM model\n",
        "model_lstm = Sequential()\n",
        "model_lstm.add(LSTM(100, input_shape=(X_train.shape[1], 1)))\n",
        "model_lstm.add(Dropout(0.5))\n",
        "model_lstm.add(Dense(32, activation='relu'))\n",
        "# Assuming num_classes is the number of unique labels in your dataset\n",
        "num_classes = len(set(y_train))\n",
        "model_lstm.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_lstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Display the model summary\n",
        "model_lstm.summary()\n",
        "\n",
        "# Train the LSTM model\n",
        "model_lstm.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2, shuffle=True)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "loss, accuracy = model_lstm.evaluate(X_test, y_test_encoded)\n",
        "print(f\"Loss: {loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Convert predictions back to original class labels\n",
        "import numpy as np\n",
        "\n",
        "# Predict probabilities for each class\n",
        "y_test_probs = model_lstm.predict(X_test)\n",
        "\n",
        "# Find the class with the highest probability for each sample\n",
        "y_test_pred = np.argmax(y_test_probs, axis=1)\n",
        "\n",
        "y_test_pred_original = label_encoder.inverse_transform(y_test_pred)\n",
        "\n",
        "# Save the trained LSTM model\n",
        "model_lstm.save('lstm_model.h5')\n",
        "print(\"LSTM model saved.\")\n",
        "\n",
        "# Save the label encoder for future use\n",
        "with open('label_encoder.pkl', 'wb') as le_file:\n",
        "    pickle.dump(label_encoder, le_file)\n",
        "\n",
        "\n",
        "# Evaluate the model performance\n",
        "precision = precision_score(y_test_encoded, y_test_pred, average='weighted')\n",
        "recall = recall_score(y_test_encoded, y_test_pred, average='weighted')\n",
        "f1 = f1_score(y_test_encoded, y_test_pred, average='weighted')\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Display confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test_encoded, y_test_pred)\n",
        "class_report = classification_report(y_test_encoded, y_test_pred)\n",
        "\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(class_report)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the label encoder for future use\n",
        "with open('label_encoder.pkl', 'wb') as le_file:\n",
        "    pickle.dump(label_encoder, le_file)"
      ],
      "metadata": {
        "id": "5q4_IyX9bgpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uapVGb8bl6nn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}