{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMZhsmawOctOcI8U6lrNRAK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/Case_study/blob/main/cnn_with_allmini.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3t1Oy9RwC5kk",
        "outputId": "a6b92fee-204d-4706-be7d-0ada09713a23"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting sentence-transformers\n",
            "  Downloading sentence_transformers-3.0.1-py3-none-any.whl (227 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/227.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━\u001b[0m \u001b[32m174.1/227.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.1/227.1 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.34.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.41.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.3.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.11.4)\n",
            "Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (0.23.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (9.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.11.0->sentence-transformers)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.11.0->sentence-transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2024.5.15)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, sentence-transformers\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105 sentence-transformers-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "# Convert 'account.type' to 0 and 1\n",
        "df['account.type'] = df['account.type'].map({'human': 0, 'bot': 1})\n",
        "\n",
        "# Filter for 1000 bots and 1000 humans\n",
        "bots = df[df['account.type'] == 1].sample(1000, random_state=42)\n",
        "humans = df[df['account.type'] == 0].sample(1000, random_state=42)\n",
        "\n",
        "# Combine the samples\n",
        "balanced_df = pd.concat([bots, humans])\n",
        "\n",
        "# Shuffle the combined DataFrame\n",
        "balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Count occurrences in the 'account.type' column\n",
        "account_type_counts = balanced_df['account.type'].value_counts()\n",
        "print(\"Account Type Counts:\")\n",
        "print(account_type_counts)\n",
        "\n",
        "# Count occurrences in the 'class_type' column\n",
        "class_type_counts = balanced_df['class_type'].value_counts()\n",
        "print(\"\\nClass Type Counts:\")\n",
        "print(class_type_counts)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kTkELvErEYM_",
        "outputId": "f3707d32-4f8e-4c8c-f83e-e8b5ec87c351"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Account Type Counts:\n",
            "account.type\n",
            "0    1000\n",
            "1    1000\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Class Type Counts:\n",
            "class_type\n",
            "human     1000\n",
            "others     385\n",
            "rnn        325\n",
            "gpt2       290\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jpFiVMTSCyg4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3b23fa19-df2e-4c12-acfc-0c43df8dbce1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 2s 16ms/step - loss: 0.6897 - accuracy: 0.5200 - val_loss: 0.6792 - val_accuracy: 0.6867\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6735 - accuracy: 0.6143 - val_loss: 0.6516 - val_accuracy: 0.7167\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.6354 - accuracy: 0.6921 - val_loss: 0.5952 - val_accuracy: 0.7533\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.5788 - accuracy: 0.7271 - val_loss: 0.5267 - val_accuracy: 0.7933\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.5319 - accuracy: 0.7600 - val_loss: 0.4774 - val_accuracy: 0.8033\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4911 - accuracy: 0.7743 - val_loss: 0.4512 - val_accuracy: 0.7900\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 0s 4ms/step - loss: 0.4572 - accuracy: 0.7936 - val_loss: 0.4370 - val_accuracy: 0.8000\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4332 - accuracy: 0.8043 - val_loss: 0.4316 - val_accuracy: 0.7867\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4082 - accuracy: 0.8279 - val_loss: 0.4272 - val_accuracy: 0.7833\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3818 - accuracy: 0.8357 - val_loss: 0.4265 - val_accuracy: 0.7833\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 0.5132 - accuracy: 0.7600\n",
            "Test accuracy: 0.7599999904632568\n",
            "10/10 [==============================] - 0s 2ms/step\n",
            "Confusion Matrix:\n",
            "[[121  35]\n",
            " [ 37 107]]\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        0          0\n",
            "1        0          0\n",
            "2        1          0\n",
            "3        1          1\n",
            "4        0          0\n",
            "5        0          0\n",
            "6        0          0\n",
            "7        0          0\n",
            "8        0          0\n",
            "9        0          0\n",
            "10       0          1\n",
            "11       0          0\n",
            "12       0          1\n",
            "13       1          0\n",
            "14       1          1\n",
            "15       0          0\n",
            "16       1          0\n",
            "17       1          0\n",
            "18       0          1\n",
            "19       1          1\n",
            "20       1          1\n",
            "21       0          0\n",
            "22       0          0\n",
            "23       1          1\n",
            "24       1          0\n",
            "25       1          0\n",
            "26       0          1\n",
            "27       1          1\n",
            "28       0          0\n",
            "29       1          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "\n",
        "X = balanced_df['text'].values\n",
        "y = balanced_df['account.type'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train, validation, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=10,\n",
        "                        batch_size=64,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = 'train.csv'\n",
        "df = pd.read_csv(csv_file_path, sep=';')\n",
        "\n",
        "# Convert 'account.type' to 0 and 1\n",
        "df['account.type'] = df['account.type'].map({'human': 0, 'bot': 1})"
      ],
      "metadata": {
        "id": "eYCIeVVwD-2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Count occurrences in the 'account.type' column\n",
        "account_type_counts = df['account.type'].value_counts()\n",
        "print(\"Account Type Counts:\")\n",
        "print(account_type_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6bivVAoH-QP",
        "outputId": "bc9be342-66f7-485f-ba53-4f7de19233a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Account Type Counts:\n",
            "account.type\n",
            "0    10358\n",
            "1    10354\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "\n",
        "X = df['text'].values\n",
        "y = df['account.type'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train, validation, test sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=10,\n",
        "                        batch_size=64,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgcdwL7NIF8q",
        "outputId": "a21b6fde-1a1e-4c2e-8ba1-d932fdf6f731"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "227/227 [==============================] - 2s 4ms/step - loss: 0.5614 - accuracy: 0.7117 - val_loss: 0.4455 - val_accuracy: 0.7895\n",
            "Epoch 2/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.4451 - accuracy: 0.7880 - val_loss: 0.4102 - val_accuracy: 0.8021\n",
            "Epoch 3/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.4081 - accuracy: 0.8086 - val_loss: 0.3940 - val_accuracy: 0.8133\n",
            "Epoch 4/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.3879 - accuracy: 0.8194 - val_loss: 0.3847 - val_accuracy: 0.8146\n",
            "Epoch 5/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.3697 - accuracy: 0.8314 - val_loss: 0.3824 - val_accuracy: 0.8114\n",
            "Epoch 6/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.3534 - accuracy: 0.8367 - val_loss: 0.3774 - val_accuracy: 0.8130\n",
            "Epoch 7/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.3375 - accuracy: 0.8478 - val_loss: 0.3738 - val_accuracy: 0.8136\n",
            "Epoch 8/10\n",
            "227/227 [==============================] - 1s 4ms/step - loss: 0.3220 - accuracy: 0.8589 - val_loss: 0.3712 - val_accuracy: 0.8198\n",
            "Epoch 9/10\n",
            "227/227 [==============================] - 1s 5ms/step - loss: 0.3105 - accuracy: 0.8609 - val_loss: 0.3678 - val_accuracy: 0.8227\n",
            "Epoch 10/10\n",
            "227/227 [==============================] - 1s 6ms/step - loss: 0.3005 - accuracy: 0.8677 - val_loss: 0.3713 - val_accuracy: 0.8256\n",
            "98/98 [==============================] - 0s 2ms/step - loss: 0.3799 - accuracy: 0.8278\n",
            "Test accuracy: 0.8278082013130188\n",
            "98/98 [==============================] - 0s 2ms/step\n",
            "Confusion Matrix:\n",
            "[[1196  363]\n",
            " [ 172 1376]]\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        1          1\n",
            "1        1          1\n",
            "2        1          0\n",
            "3        0          0\n",
            "4        0          0\n",
            "5        1          0\n",
            "6        0          0\n",
            "7        0          1\n",
            "8        1          1\n",
            "9        1          1\n",
            "10       1          0\n",
            "11       0          0\n",
            "12       1          1\n",
            "13       0          0\n",
            "14       1          1\n",
            "15       1          1\n",
            "16       1          1\n",
            "17       0          0\n",
            "18       1          0\n",
            "19       1          1\n",
            "20       0          1\n",
            "21       1          1\n",
            "22       1          1\n",
            "23       1          0\n",
            "24       0          0\n",
            "25       1          1\n",
            "26       0          1\n",
            "27       1          1\n",
            "28       1          1\n",
            "29       1          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "csv_file_path = 'test.csv'\n",
        "df_test = pd.read_csv(csv_file_path, sep=';')\n",
        "df_test.head()\n",
        "\n",
        "# Convert 'account.type' to 0 and 1\n",
        "df_test['account.type'] = df_test['account.type'].map({'human': 0, 'bot': 1})\n",
        "\n",
        "# Count occurrences in the 'account.type' column\n",
        "account_type_counts = df['account.type'].value_counts()\n",
        "print(\"Account Type Counts:\")\n",
        "print(account_type_counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rScP3Om7Ica4",
        "outputId": "25f5a260-56e0-461b-d5cc-45950e2ff767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Account Type Counts:\n",
            "account.type\n",
            "bot      1280\n",
            "human    1278\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load MiniLM model\n",
        "model_name = 'all-MiniLM-L6-v2'\n",
        "model = SentenceTransformer(model_name)\n",
        "\n",
        "# Step 1: Read and Prepare Data\n",
        "\n",
        "X = balanced_df['text'].values\n",
        "y = balanced_df['account.type'].values\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(y)\n",
        "\n",
        "# Splitting data into train and validation sets only\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Step 2: Get MiniLM Embeddings\n",
        "def get_minilm_embeddings(model, texts):\n",
        "    embeddings = model.encode(texts, convert_to_tensor=True)\n",
        "    return embeddings.cpu().numpy()  # Convert to NumPy array and move to CPU\n",
        "\n",
        "X_train_embeddings = get_minilm_embeddings(model, X_train)\n",
        "X_val_embeddings = get_minilm_embeddings(model, X_val)\n",
        "\n",
        "# Step 3: Define and compile your model with MiniLM Embeddings\n",
        "input_shape = X_train_embeddings.shape[1:]\n",
        "\n",
        "model_cnn = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=input_shape),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model_cnn.compile(optimizer='adam',\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "# Step 4: Training\n",
        "history = model_cnn.fit(X_train_embeddings, y_train,\n",
        "                        epochs=10,\n",
        "                        batch_size=64,\n",
        "                        validation_data=(X_val_embeddings, y_val),\n",
        "                        callbacks=[tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)])\n",
        "\n",
        "# Step 5: Load and Prepare Test Data\n",
        "test_df = pd.read_csv('test.csv', sep=';')\n",
        "test_df['account.type'] = test_df['account.type'].map({'human': 0, 'bot': 1})\n",
        "X_test = test_df['text'].values\n",
        "y_test = test_df['account.type'].values  # Use the values directly\n",
        "\n",
        "X_test_embeddings = get_minilm_embeddings(model, X_test)\n",
        "\n",
        "# Step 6: Evaluation\n",
        "loss, accuracy = model_cnn.evaluate(X_test_embeddings, y_test)\n",
        "print(f'Test accuracy: {accuracy}')\n",
        "\n",
        "# Confusion Matrix\n",
        "y_pred = np.argmax(model_cnn.predict(X_test_embeddings), axis=1)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion Matrix:')\n",
        "print(cm)\n",
        "\n",
        "# Classification Report\n",
        "report = classification_report(y_test, y_pred, target_names=['human', 'bot'])\n",
        "print('Classification Report:')\n",
        "print(report)\n",
        "\n",
        "# Print Actual vs Predicted Outputs\n",
        "actual_vs_predicted = pd.DataFrame({'Actual': y_test, 'Predicted': y_pred})\n",
        "print(\"Actual vs Predicted Outputs:\")\n",
        "print(actual_vs_predicted.head(30))  # Display the first 30 predictions\n",
        "\n",
        "# Save the model\n",
        "model_cnn.save('text_classification_model_with_minilm_cnn.h5')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B7AwAN49ILoO",
        "outputId": "2f0ed31b-82eb-44fa-86dc-3e1c964709fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "22/22 [==============================] - 2s 17ms/step - loss: 0.6867 - accuracy: 0.5586 - val_loss: 0.6685 - val_accuracy: 0.6933\n",
            "Epoch 2/10\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6542 - accuracy: 0.6557 - val_loss: 0.6255 - val_accuracy: 0.7267\n",
            "Epoch 3/10\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.6044 - accuracy: 0.7121 - val_loss: 0.5634 - val_accuracy: 0.7550\n",
            "Epoch 4/10\n",
            "22/22 [==============================] - 0s 8ms/step - loss: 0.5506 - accuracy: 0.7443 - val_loss: 0.5172 - val_accuracy: 0.7533\n",
            "Epoch 5/10\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4983 - accuracy: 0.7657 - val_loss: 0.4876 - val_accuracy: 0.7700\n",
            "Epoch 6/10\n",
            "22/22 [==============================] - 0s 9ms/step - loss: 0.4642 - accuracy: 0.7843 - val_loss: 0.4797 - val_accuracy: 0.7750\n",
            "Epoch 7/10\n",
            "22/22 [==============================] - 0s 6ms/step - loss: 0.4392 - accuracy: 0.7993 - val_loss: 0.4699 - val_accuracy: 0.7783\n",
            "Epoch 8/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.4178 - accuracy: 0.8043 - val_loss: 0.4657 - val_accuracy: 0.7700\n",
            "Epoch 9/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3847 - accuracy: 0.8421 - val_loss: 0.4666 - val_accuracy: 0.7783\n",
            "Epoch 10/10\n",
            "22/22 [==============================] - 0s 5ms/step - loss: 0.3667 - accuracy: 0.8529 - val_loss: 0.4722 - val_accuracy: 0.7767\n",
            "80/80 [==============================] - 0s 2ms/step - loss: 0.4997 - accuracy: 0.7615\n",
            "Test accuracy: 0.7615324258804321\n",
            "80/80 [==============================] - 0s 1ms/step\n",
            "Confusion Matrix:\n",
            "[[ 939  339]\n",
            " [ 271 1009]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       human       0.78      0.73      0.75      1278\n",
            "         bot       0.75      0.79      0.77      1280\n",
            "\n",
            "    accuracy                           0.76      2558\n",
            "   macro avg       0.76      0.76      0.76      2558\n",
            "weighted avg       0.76      0.76      0.76      2558\n",
            "\n",
            "Actual vs Predicted Outputs:\n",
            "    Actual  Predicted\n",
            "0        0          1\n",
            "1        0          0\n",
            "2        0          0\n",
            "3        1          1\n",
            "4        0          0\n",
            "5        1          1\n",
            "6        1          1\n",
            "7        0          1\n",
            "8        0          0\n",
            "9        0          0\n",
            "10       0          0\n",
            "11       1          1\n",
            "12       0          0\n",
            "13       1          1\n",
            "14       0          1\n",
            "15       0          0\n",
            "16       0          1\n",
            "17       1          1\n",
            "18       0          0\n",
            "19       1          1\n",
            "20       0          0\n",
            "21       1          0\n",
            "22       1          1\n",
            "23       0          0\n",
            "24       0          0\n",
            "25       1          1\n",
            "26       0          1\n",
            "27       1          1\n",
            "28       1          1\n",
            "29       1          1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w0v8wkKPJH7-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}