{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Meenusj/Case_study/blob/main/test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AmAlLJUBdGQO",
        "outputId": "38ddbcbb-fbc6-421c-c33b-5885166f0a64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/68.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pybind11>=2.2 (from fasttext)\n",
            "  Using cached pybind11-2.11.1-py3-none-any.whl (227 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext) (67.7.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fasttext) (1.25.2)\n",
            "Building wheels for collected packages: fasttext\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4199775 sha256=9491d8619f9e8032b3e0d3cbadf203729e6265b96d281c784f81260d83cefb5d\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built fasttext\n",
            "Installing collected packages: pybind11, fasttext\n",
            "Successfully installed fasttext-0.9.2 pybind11-2.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install fasttext"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "!gzip -d cc.en.300.bin.gz\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzFUhVobZ1Ae",
        "outputId": "146d2f72-3099-47ca-bd61-d6fded15c270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-02-18 13:28:42--  https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
            "Resolving dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)... 13.227.219.59, 13.227.219.33, 13.227.219.70, ...\n",
            "Connecting to dl.fbaipublicfiles.com (dl.fbaipublicfiles.com)|13.227.219.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4503593528 (4.2G) [application/octet-stream]\n",
            "Saving to: ‘cc.en.300.bin.gz’\n",
            "\n",
            "cc.en.300.bin.gz    100%[===================>]   4.19G  87.2MB/s    in 31s     \n",
            "\n",
            "2024-02-18 13:29:13 (139 MB/s) - ‘cc.en.300.bin.gz’ saved [4503593528/4503593528]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1bt9O0kXVck4",
        "outputId": "f079515f-6746-4ef2-9a3a-68e2f7faa03b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1218/1218 [==============================] - 4s 2ms/step\n",
            "New Dataset Evaluation Metrics:\n",
            "Accuracy: 0.9344\n",
            "Precision: 0.8730\n",
            "Recall: 0.9344\n",
            "F1 Score: 0.9027\n",
            "Confusion Matrix for the New Dataset:\n",
            "[[    0     0     0     0   384]\n",
            " [    0     0     0     0  1278]\n",
            " [    0     0     0     0   484]\n",
            " [    0     0     0     0   412]\n",
            " [    0     0     0     0 36412]]\n",
            "Classification Report for the New Dataset:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       384\n",
            "           1       0.00      0.00      0.00      1278\n",
            "           2       0.00      0.00      0.00       484\n",
            "           3       0.00      0.00      0.00       412\n",
            "           4       0.93      1.00      0.97     36412\n",
            "\n",
            "    accuracy                           0.93     38970\n",
            "   macro avg       0.19      0.20      0.19     38970\n",
            "weighted avg       0.87      0.93      0.90     38970\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import load_model\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import fasttext\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "\n",
        "# Load the FastText model\n",
        "model = fasttext.load_model('cc.en.300.bin')\n",
        "\n",
        "\n",
        "# Assuming the correct column name is 'text', replace 'tweet_text' with the actual name\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Case conversion\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "\n",
        "    # Remove hashtags and usernames\n",
        "    tokens = [re.sub(r'#\\w+|@\\w+', '', word) for word in tokens]\n",
        "\n",
        "    # Remove punctuation\n",
        "    tokens = [word for word in tokens if word.isalnum()]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Load the pre-trained CNN model\n",
        "loaded_model = load_model('/content/deepfake_model (1).h5')\n",
        "\n",
        "# Load the label encoder\n",
        "with open('/content/label_encoder (2).pkl', 'rb') as le_file:\n",
        "    label_encoder = pickle.load(le_file)\n",
        "\n",
        "\n",
        "\n",
        "# Load your new dataset (replace 'new_test.csv' with your actual dataset file)\n",
        "new_csv_file_path = 'test.csv'\n",
        "new_df = pd.read_csv(new_csv_file_path, delimiter=';')\n",
        "\n",
        "# Preprocess the text in the 'text' column\n",
        "new_df['preprocessed_text'] = new_df['text'].apply(preprocess)\n",
        "\n",
        "# Tokenize the text\n",
        "new_tokenized_text = [word_tokenize(text) for text in new_df['preprocessed_text']]\n",
        "\n",
        "# Get word vectors for each token using the pre-trained FastText model\n",
        "new_word_vectors = [model.get_word_vector(word) for tokens in new_tokenized_text for word in tokens]\n",
        "\n",
        "# Convert word vectors to DataFrame\n",
        "new_word_vectors_df = pd.DataFrame(new_word_vectors, columns=[f'feature_{i}' for i in range(300)])\n",
        "\n",
        "# Concatenate the original DataFrame with the word vectors DataFrame\n",
        "new_df_with_vectors = pd.concat([new_df, new_word_vectors_df], axis=1)\n",
        "\n",
        "# Extract feature columns (assuming they start from column 'feature_0')\n",
        "new_feature_columns = new_df_with_vectors.columns[new_df_with_vectors.columns.str.startswith('feature_')]\n",
        "\n",
        "# Extract features and labels\n",
        "new_X = new_df_with_vectors[new_feature_columns].values\n",
        "new_y_actual = label_encoder.transform(new_df_with_vectors['class_type'])\n",
        "\n",
        "# Reshape the input data to be compatible with Conv1D layer\n",
        "new_X = new_X.reshape(new_X.shape[0], new_X.shape[1], 1)\n",
        "\n",
        "# Predict on the new dataset\n",
        "new_y_pred_probs = loaded_model.predict(new_X)\n",
        "new_y_pred_classes = new_y_pred_probs.argmax(axis=-1)\n",
        "\n",
        "# Calculate evaluation metrics for the new dataset\n",
        "new_accuracy = accuracy_score(new_y_actual, new_y_pred_classes)\n",
        "new_precision = precision_score(new_y_actual, new_y_pred_classes, average='weighted')\n",
        "new_recall = recall_score(new_y_actual, new_y_pred_classes, average='weighted')\n",
        "new_f1 = f1_score(new_y_actual, new_y_pred_classes, average='weighted')\n",
        "\n",
        "# Print the evaluation metrics for the new dataset\n",
        "print(f\"New Dataset Evaluation Metrics:\")\n",
        "print(f\"Accuracy: {new_accuracy:.4f}\")\n",
        "print(f\"Precision: {new_precision:.4f}\")\n",
        "print(f\"Recall: {new_recall:.4f}\")\n",
        "print(f\"F1 Score: {new_f1:.4f}\")\n",
        "\n",
        "# Create a confusion matrix for the new dataset\n",
        "new_conf_matrix = confusion_matrix(new_y_actual, new_y_pred_classes)\n",
        "\n",
        "# Print the confusion matrix for the new dataset\n",
        "print(\"Confusion Matrix for the New Dataset:\")\n",
        "print(new_conf_matrix)\n",
        "\n",
        "# Create a classification report for the new dataset\n",
        "new_class_report = classification_report(new_y_actual, new_y_pred_classes)\n",
        "\n",
        "# Print the classification report for the new dataset\\]\n",
        "\n",
        "print(\"Classification Report for the New Dataset:\")\n",
        "print(new_class_report)\n",
        "\n",
        "\n",
        "# # Print actual vs predicted output\n",
        "# for actual, predicted in zip(new_y_actual, new_y_pred_classes):\n",
        "#     print(f\"Actual: {label_encoder.inverse_transform([actual])[0]}, Predicted: {label_encoder.inverse_transform([predicted])[0]}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPu4HR1oc8giO1JhVoWytUx",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}